---
title: "Project 2"
author: "Group 6, Xavier Genelin, Dave Bergeron"
date: "10/19/2021"
output: github_document
---

I don't think we need all of these libraries. We can keep them for now but the project doesn't require anything with SQL so those can be removed later.

Dave:  Agree, one of my bad habits not pruning the list from one assignment to the other.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(rmarkdown)
library(haven)
library(readxl)
library(parallel)
library(purrr)
library(lattice)
library(caret)
library(ciTools)
library(plot3D)
library(MuMIn)
library(devtools)
library(pls)
library(corrplot)
library(car)
library(class)
library(gbm)
library(randomForest)
```

# Introduction

We'll load in the (Online News Popularity Data Set)[https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity]  from the UCI Machine Learning Repository. From this data, we'll break it down into 6 different groups based on the data channel: lifestyle, entertainment, bus, socmed, tech, and world. 


# Load Data w/Automation
```{r}
# load in the data
news <- read_csv("OnlineNewsPopularity.csv")

#### this will be part of the overall parameters but we can change this as we go forward for different datasets
channel <- "world"

# create a new column for the data channel
news <- news %>% 
 mutate(data_channel = 
          if_else(data_channel_is_lifestyle == 1, "lifestyle", 
                  if_else(data_channel_is_entertainment == 1, "entertainment", 
                          if_else(data_channel_is_bus == 1, "bus", 
                                  if_else(data_channel_is_socmed == 1, "socmed", 
                                          if_else(data_channel_is_tech == 1, "tech", 
                                                  if_else(data_channel_is_world == 1, "world", "other")))))))

# Filter the news data set on the channel of interest
# remove the url and data_channel_is_* columns
news <- news %>% filter(data_channel == channel) %>% select(!c(url, starts_with("data_channel")))

# set seed for reproducibility
set.seed(55)

# create a list of indicies for the training set
trainIndex <- createDataPartition(news$shares, p = 0.7, list = FALSE)

# make training and test sets
newsTrain <- news[trainIndex, ]
newsTest <- news[-trainIndex, ]
```

# Summarizations

I was hoping to see the correlation plot for the variables to see what was most related to shares. Kind of rough with the large number of variables

```{r}
corrs <- cor(newsTrain)

corrplot(corrs, tl.cex = 0.5)
```


# EDA items

## Graphs

### Graph 1

```{r}
# still working on this...need to play around with the data a bit more.
p <- ggplot(worldTrain, aes(x=shares)) + 
  geom_boxplot() + coord_flip()
p
```


### Graph 2

### Graph 3

## Contingency Tables

### 2-way Contingency Table

Shows token count in titles on Mondays 

```{r, eval=TRUE, echo=TRUE}
table(newsTrain$n_tokens_title, newsTrain$weekday_is_monday)
```

### 3-Way Contingency Table

xxx

```{r, eval=TRUE, echo=TRUE}

```



# Modeling

In this section we're going to compare the following models: random forest, linear regression, and two ensemble models (we can fill this in when we decide which two to do). Each of the models will be trying to predict the amount of shares for each of the data channels. 



## Random Forest

Tried to use all the variables, took forever to run. Initially had 10 cv and 1:15 trees. Made the cv to 5 and tried smaller mtry values but it still takes a long time. 

```{r}
set.seed(55)
# takes forever to run
# rfFit <- train(shares ~ ., data = worldTrain, method = "rf", trControl = trainControl(method = "cv", number = 5), tuneGrid = expand.grid(mtry = 1:8))
```

## Linear Regression
```{r}
#Model 1 -  Selecting predictors based on relevancy from ANOVA results after running the summary from model using all predictors in another model - 4 predictors
worldtrainglmfit <- lm(shares ~ num_imgs + kw_min_avg + kw_max_avg + kw_avg_avg, data = newsTrain, trControl = trainControl(method = "cv", number = 10), preProcess = c("center", "scale"))

pred1 <- predict(worldtrainglmfit, newdata = newsTest)

postResample(pred1, obs = newsTest$shares)
```

```{r}
# Xavier model
olsFit <- train(shares ~ ., data = newsTrain, trControl = trainControl(method = "cv", number = 5))

olsPred <- predict(olsFit, newsTest)

postResample(olsPred, obs = newsTest$shares)[2]
```

## Ensemble 

### Boosted Tree

This is the boosted tree model using all predictors in the `worldTrain` data set.

```{r}
set.seed(30)
wtFit <- train(shares ~ ., data = newsTrain,
               method = "gbm",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv",
                                        number = 5),
               tuneGrid = expand.grid(.n.trees = seq(25, 200, by = 25), .interaction.depth = seq(1, 4, by = 1), .shrinkage = (0.1), .n.minobsinnode = (10)))
wtFit
```

#### Boosted Tree Test Results

xxxx

```{r, eval = FALSE}

```
# Comparison


## Test for project 2

Test

```{r, echo=TRUE, eval=FALSE}

```


