---
title: "Project 2"
author: "Group 6, Xavier Genelin, Dave Bergeron"
date: "10/19/2021"
output: 
  github_document:
    html_preview: FALSE
    toc: TRUE
#params:
  #channel: channel
---

I don't think we need all of these libraries. We can keep them for now but the project doesn't require anything with SQL so those can be removed later.

Dave:  Agree, one of my bad habits not pruning the list from one assignment to the other.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(rmarkdown)
library(haven)
library(readxl)
library(parallel)
library(purrr)
library(lattice)
library(caret)
library(ciTools)
library(plot3D)
library(MuMIn)
library(devtools)
library(pls)
library(corrplot)
library(car)
library(class)
library(gbm)
library(randomForest)
```

# Introduction

We'll load in the (Online News Popularity Data Set)[https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity]  from the UCI Machine Learning Repository. From this data, we'll break it down into 6 different groups based on the data channel: lifestyle, entertainment, bus, socmed, tech, and world. 

Libraries that are being used:  

* `tidyverse`  
* `caret`  


# Load Data w/Automation
```{r}
# load in the data
news <- read_csv("OnlineNewsPopularity.csv")

#### this will be part of the overall parameters but we can change this as we go forward for different datasets
channel <- "world"

# create a new column for the data channel
news <- news %>% 
 mutate(data_channel = 
          if_else(data_channel_is_lifestyle == 1, "lifestyle", 
                  if_else(data_channel_is_entertainment == 1, "entertainment", 
                          if_else(data_channel_is_bus == 1, "bus", 
                                  if_else(data_channel_is_socmed == 1, "socmed", 
                                          if_else(data_channel_is_tech == 1, "tech", 
                                                  if_else(data_channel_is_world == 1, "world", "other")))))),
        weekday = if_else(weekday_is_monday == 1, "Monday", 
                          if_else(weekday_is_tuesday == 1, "Tuesday",
                                  if_else(weekday_is_wednesday == 1, "Wednesday",
                                          if_else(weekday_is_thursday == 1, "Thursday",
                                                  if_else(weekday_is_friday == 1, "Friday",
                                                          if_else(weekday_is_saturday == 1, "Saturday", "Sunday"))))))
        )

# Filter the news data set on the channel of interest
# remove the url and data_channel_is_* and weekday_is_* columns
news <- news %>% filter(data_channel == channel) %>% select(!c(url, starts_with("data_channel"), starts_with("weekday_is")))

news$weekday <- factor(news$weekday, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

# set seed for reproducibility
set.seed(55)

# create a list of indicies for the training set
trainIndex <- createDataPartition(news$shares, p = 0.7, list = FALSE)

# make training and test sets
newsTrain <- news[trainIndex, ]
newsTest <- news[-trainIndex, ]
```

# Summarizations

I was hoping to see the correlation plot for the variables to see what was most related to shares. Kind of rough with the large number of variables

```{r}
corrs <- cor(worldTrain)

corrplot(corrs, tl.cex = 0.5)
```


# EDA items

## Graphs

### Graph 1

```{r}
ggplot(data = newsTrain, aes(x = newsTrain$num_keywords, y = newsTrain$num_imgs)) + geom_point(aes(color = newsTrain$shares), position = "jitter") + labs(x = "Keywords", y = "Number of Images", title="Images to Keywords ")
```


### Graph 2
```{r}
# xavier. Not the best but I'll try and make it be a little more detailed
ggplot(data = newsTrain, aes(x = ordered(weekday, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")), y = shares)) + 
  geom_col() + 
  labs(title = "Shares per Day", x = "Day", y = "Shares")
```



### Graph 3

```{r}
# xavier
ggplot(newsTrain, aes(x = num_imgs, y = shares)) +
  geom_point(aes(color = weekday)) +
  labs(title = "Shares vs Number of Images", x = "Number of Images", y = "Shares")
```
### Graph 4

```{r}
ggplot(data = newsTrain, aes(y = rate_positive_words, x = global_subjectivity)) + geom_point(aes(color = rate_positive_words), position = "jitter") + geom_smooth(formula = y ~ x, method = "loess") + labs(x = "Global Subjectivity", y = "Rate Positive Words", title="Correlation of Global Subjectivity to Rate of Positive Words")
```


### Graph 5

```{r}
ggplot(newsTrain, aes(x=timedelta)) + 
  geom_line(aes(y=shares))+ labs(title="Shares across timedelta",
       y="Shares")
```

### Graph 6

```{r}

```


## Contingency Tables

### 2-way Contingency Table

Showing counts of keywords that appear on the weekend vs. not on the weekend.

```{r, eval=TRUE, echo=TRUE}
table(newsTrain$num_keywords, newsTrain$is_weekend)
```

### 3-Way Contingency Table

I need to figure out what a kw_min_min means, but it works for the three way table given the fixed number of categories.

```{r, eval=TRUE, echo=TRUE}
table(newsTrain$num_keywords, newsTrain$is_weekend, newsTrain$kw_min_min)
```


## Numerical Summary
```{r}
newsTrain %>% group_by(weekday) %>% summarise(Min = min(shares), Mean = mean(shares), Max = max(shares), SD = sd(shares))
```



# Modeling

In this section we're going to compare the following models: random forest, linear regression, and two ensemble models (we can fill this in when we decide which two to do). Each of the models will be trying to predict the amount of shares for each of the data channels. 



## Random Forest

Tried to use all the variables, took forever to run. Initially had 10 cv and 1:15 trees. Made the cv to 5 and tried smaller mtry values but it still takes a long time. 

```{r}
set.seed(55)
now <- Sys.time()
# takes forever to run
rfFit <- train(shares ~ num_imgs + kw_avg_avg + LDA_00 + LDA_01 + LDA_02 + LDA_03 + LDA_04, data = newsTrain, method = "rf", preProcess = c("center", "scale"), trControl = trainControl(method = "cv", number = 5), tuneGrid = expand.grid(mtry = 1:8))

rfFit$results
# took 14 minutes
Sys.time() - now
```

## Linear Regression
```{r}
#Model 1 -  Selecting predictors based on relevancy from ANOVA results after running the summary from model using all predictors in another model - 4 predictors
worldtrainglmfit <- lm(shares ~ num_imgs + kw_min_avg + kw_max_avg + kw_avg_avg, data = newsTrain, trControl = trainControl(method = "cv", number = 10), preProcess = c("center", "scale"))

pred1 <- predict(worldtrainglmfit, newdata = newsTest)

postResample(pred1, obs = newsTest$shares)[2]
```

```{r}
# Xavier model
olsFit <- train(shares ~  num_imgs + kw_avg_avg + LDA_02 + LDA_03 + average_token_length + rate_negative_words, data = newsTrain, method = "lm", preProcess = c("center", "scale"), trControl = trainControl(method = "cv", number = 10))

olsPred <- predict(olsFit, newsTest)

postResample(olsPred, obs = newsTest$shares)[2]

```

```{r}
# poisson model
poissonFit <- train(shares ~ num_imgs + kw_avg_avg + LDA_02 + LDA_03 + average_token_length + rate_negative_words, data = newsTrain, method = "glm", family = "poisson", preProcess = c("center", "scale"), trControl = trainControl(method = "cv", number = 10))

poissonPred <- predict(poissonFit, newsTest)

postResample(poissonPred, obs = newsTest$shares)[2]
```

Not much of a difference between the linear model and poisson

## Ensemble 

### Boosted Tree

This is the boosted tree model using all predictors in the `newsTrain` data set.

```{r}
set.seed(30)
wtFit <- train(shares ~ num_imgs + kw_avg_avg + LDA_02 + LDA_03 + average_token_length + rate_negative_words, data = newsTrain,
               method = "gbm",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv",
                                        number = 5),
               tuneGrid = expand.grid(.n.trees = seq(25, 200, by = 25), .interaction.depth = seq(1, 4, by = 1), .shrinkage = (0.1), .n.minobsinnode = (10)))
wtFit
```

#### Boosted Tree Test Results

Still working on this,getting an error

```{r, eval = FALSE}

```
# Comparison


## Test for project 2

Test

```{r, echo=TRUE, eval=FALSE}
library(lares)
corr_var(news, shares)
```


