---
title: "`r params$channel` Analysis"
author: "Group 6, Xavier Genelin, Dave Bergeron"
date: "10/27/2021"
output: 
  github_document:
    html_preview: FALSE
    toc: TRUE
params:
  channel: world
---

I don't think we need all of these libraries. We can keep them for now but the project doesn't require anything with SQL so those can be removed later.

Dave:  Agree, one of my bad habits not pruning the list from one assignment to the other.

Xavier: I've added a comment to each of them that I've used

```{r setup, include=FALSE}
imagePath <- paste0("../Reports/", params$channel, "_files/figure-gfm")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.path = imagePath)
library(tidyverse) # used
library(knitr)
library(rmarkdown)
library(haven)
library(readxl)
library(parallel) # used
library(purrr)
library(lattice)
library(caret)
library(ciTools)
library(plot3D)
library(MuMIn)
library(devtools)
library(pls)
library(corrplot) # used
library(car)
library(class)
library(gbm)
library(randomForest)
library(doParallel) # used
```

# Introduction

For this project, we'll be analyzing the(Online News Popularity Data Set)[https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity] from the UCI Machine Learning Repository. For this analysis, we'll be examining the `r params$channel` data channel. Our analysis for this channel will go into an exploratory data analysis with different graphs and numerical summaries, as well as trying to predict the number of shares with different types of models. 

Libraries that are being used:  

* `tidyverse`  
* `caret`  
* `corrplot`  
* `doParellel`  


# Load Data w/Automation

Before doing our analysis, we need to load in the data and do some data manipulation. Since we are only looking at the `r params$channel` data channel, we'll filter the dataset on this channel and create a new variable for the weekday from the columns `weekday_is_*`. After filtering and creating the weekday variable, we'll remove the columns that went into creating those and the url column since these won't help further in our analysis. From the filtered data set we made a training and test set with a 70:30 split to help with our modeling later. 

```{r data}
# load in the data
news <- read_csv("OnlineNewsPopularity.csv")

#### this will be part of the overall parameters but we can change this as we go forward for different datasets
channel <- params$channel

# create a new column for the data channel and weekday
news <- news %>% 
 mutate(data_channel = 
          if_else(data_channel_is_lifestyle == 1, "Lifestyle", 
                  if_else(data_channel_is_entertainment == 1, "Entertainment", 
                          if_else(data_channel_is_bus == 1, "Bus", 
                                  if_else(data_channel_is_socmed == 1, "Socmed", 
                                          if_else(data_channel_is_tech == 1, "Tech", 
                                                  if_else(data_channel_is_world == 1, "World", "other")))))),
        weekday = if_else(weekday_is_monday == 1, "Monday", 
                          if_else(weekday_is_tuesday == 1, "Tuesday",
                                  if_else(weekday_is_wednesday == 1, "Wednesday",
                                          if_else(weekday_is_thursday == 1, "Thursday",
                                                  if_else(weekday_is_friday == 1, "Friday",
                                                          if_else(weekday_is_saturday == 1, "Saturday", "Sunday"))))))
        )

news$weekday <- factor(news$weekday, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

# Filter the news data set on the channel of interest
# remove the url and data_channel_is_* and weekday_is_* columns
news <- news %>% filter(data_channel == channel) %>% select(!c(url, starts_with("data_channel"), starts_with("weekday_is")))

# set seed for reproducibility
set.seed(55)

# create a list of indicies for the training set
trainIndex <- createDataPartition(news$shares, p = 0.7, list = FALSE)

# make training and test sets
newsTrain <- news[trainIndex, ]
newsTest <- news[-trainIndex, ]
```

# Summarizations

I was hoping to see the correlation plot for the variables to see what was most related to shares. Kind of rough with the large number of variables

## Graph 1

```{r graph1}
ggplot(data = newsTrain, aes(x = num_keywords, y = num_imgs)) + 
  geom_point(aes(color = shares), position = "jitter") + 
  labs(x = "Keywords", y = "Number of Images", title="Images to Keywords", colour = "Shares")
```


## Graph 2

The plot below shows boxplots of the shares by weekday. The box itself shows the middle 50$\%$ of the data for that weekday and any outliers are shown as points outside of the box. The bottom end of the box shows the number of shares that are at 25$\%$ for that weekday, the top end shows the 75th percentile of the data, and the line in the box shows the median. This can help us tell if the data is skewed one way or another based on where the line is within the box. We also added data points in blue that show the average value based on the weekday as well to compare it with the boxplot.  

```{r graph2}
# get the average shares by weekday
avgValues <- newsTrain %>% group_by(weekday) %>% summarise(avg = mean(shares)) 

ggplot(newsTrain, aes(x = weekday, y = shares)) +
  geom_boxplot(fill = "grey") + 
  coord_cartesian(ylim = c(0,10000)) +
  geom_point(avgValues, mapping = aes(x = weekday, y = avg), color = "navy") + 
  geom_line(avgValues, mapping = aes(x = weekday, y = avg, group = 1), color = "navy") +
  labs(title = "Shares by Weekday", subtitle = "Means shown in navy blue", x = "Weekday", y = "Shares")
```

## Graph 3

The next graph shows the number of images and the number of shares that we have in the data set, along with the different days in different colors. This can give us some insight in a more detailed level if there is any relationship with shares and images as a whole, or even if it depends on the day. If there is a positive relationship between the two, we'd expect to see that as images increase so does shares. If there's a negative relationship, as shares increase the number of images would go down or vice versa.  

```{r graph3}
ggplot(newsTrain, aes(x = num_imgs, y = shares)) +
  geom_point(aes(color = weekday)) +
  labs(title = "Shares vs Number of Images by Weekday", x = "Number of Images", y = "Shares")
```
## Graph 4

```{r graph4}
ggplot(data = newsTrain, aes(y = rate_positive_words, x = global_subjectivity)) + 
  geom_point(aes(color = rate_positive_words), position = "jitter") + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(x = "Global Subjectivity", y = "Rate Positive Words", 
       title="Correlation of Global Subjectivity to Rate of Positive Words", colour = "Rate Positive Words")
```


## Graph 5

```{r graph5}
ggplot(newsTrain, aes(x=timedelta)) + 
  geom_line(aes(y=shares)) + 
  labs(title="Shares across timedelta", y="Shares")
```

## Graph 6

The last graph we'll examine is a correlation plot between all the variables. The correlation will tell us how strong the relationship between two variables is, and whether that relationship is positive or negative. The color index will show the degree of the relationship between the two. The darker the color is, the stronger the relationship between those two variables. Our main variable of interest is `shares` so we're hoping to see some variables having a strong relationship in either direction with that.

```{r graph6}
# remove the weekday column since this isn't numeric after mutating it
corrs <- cor(newsTrain %>% select(!weekday))

corrplot(corrs, method =  "color", tl.cex = 0.5, type = "upper")
```


## Contingency Tables

### 2-way Contingency Table

Showing counts of keywords that appear on the weekend vs. not on the weekend.

```{r cont2, eval=TRUE, echo=TRUE}
table(newsTrain$num_keywords, newsTrain$is_weekend)
```

### 3-Way Contingency Table

I need to figure out what a kw_min_min means, but it works for the three way table given the fixed number of categories.

```{r cont3, eval=TRUE, echo=TRUE}
table(newsTrain$num_keywords, newsTrain$is_weekend, newsTrain$kw_min_min)
```


## Numerical Summary

Earlier we saw the boxplots show the number of shares based on weekdays, but with the large values it could be difficult to see where specific points lie on the graph, like the minimum value. We may not even have seen the maximum value for some of them due to large outliers. The table below will give the numeric summary of what we saw within the boxplots to help give some values to what we saw visually earlier. 

```{r numSum}
# Shares by weekday
newsTrain %>% 
  group_by(weekday) %>% 
  summarise(Min = min(shares), Q1 = quantile(shares, 0.25), Mean = mean(shares), 
            Median = median(shares), Q3 = quantile(shares, 0.75), Max = max(shares), SD = sd(shares))
```


# Modeling

In this section we're going to compare the following models: random forest, linear regression, and two ensemble models (we can fill this in when we decide which two to do). Each of the models will be trying to predict the amount of shares for each of the data channels. 

## Linear Regression


```{r lm1}
#Model 1 -  Selecting predictors based on relevancy from ANOVA results after running the summary from model using all predictors in another model - 4 predictors
worldtrainglmfit <- lm(shares ~ num_imgs + kw_min_avg + kw_max_avg + kw_avg_avg, data = newsTrain, trControl = trainControl(method = "cv", number = 10), preProcess = c("center", "scale"))

pred1 <- predict(worldtrainglmfit, newdata = newsTest)

lm1Results <- postResample(pred1, obs = newsTest$shares)
```

```{r lm2}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

mlrFit <- train(shares ~  num_imgs + kw_avg_avg + LDA_02 + LDA_03 + average_token_length + rate_negative_words, data = newsTrain, method = "lm", trControl = trainControl(method = "cv", number = 10), preProcess = c("center", "scale"))

mlrPred <- predict(mlrFit, newsTest)

mlrResults <- postResample(mlrPred, obs = newsTest$shares)

stopCluster(cl)
```

## Ensemble 

### Random Forest


```{r randForest}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

rfFit <- train(shares ~ num_imgs + kw_avg_avg + LDA_02 + LDA_03 + average_token_length + rate_negative_words, 
               data = newsTrain, method = "rf", 
               preProcess = c("center", "scale"), 
               trControl = trainControl(method = "cv", number = 5), 
               tuneGrid = expand.grid(mtry = 1:7))

rfPred <- predict(rfFit, newsTest)

rfResults <- postResample(rfPred, obs = newsTest$shares)
#rfFit$bestTune best mtry value
stopCluster(cl)
```

### Boosted Tree

This is the boosted tree model using all predictors in the `newsTrain` data set.

```{r bootTree}
# set.seed(30)
# wtFit <- train(shares ~ num_imgs + kw_avg_avg + LDA_02 + LDA_03 + average_token_length + rate_negative_words, 
#                data = newsTrain, method = "gbm", 
#                preProcess = c("center", "scale"), 
#                trControl = trainControl(method = "cv",number = 5), 
#                tuneGrid = expand.grid(.n.trees = seq(25, 200, by = 25), 
#                                       .interaction.depth = seq(1, 4, by = 1), 
#                                       .shrinkage = (0.1), 
#                                       .n.minobsinnode = (10)))
# wtFit
```

#### Boosted Tree Test Results

Still working on this,getting an error

# Comparison

```{r comparison}
# started the comparison with a table
data.frame(lm1Results, mlrResults, rfResults)
```






